{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe7ab4c",
   "metadata": {},
   "source": [
    "# HPC Job Management System - Comprehensive Guide\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive analysis of High-Performance Computing (HPC) job management, including:\n",
    "- Resource allocation strategies\n",
    "- Job scheduling algorithms\n",
    "- Performance monitoring and metrics\n",
    "- Benchmarking methodologies\n",
    "- Parallel execution strategies\n",
    "- Advanced optimization techniques\n",
    "\n",
    "**Target System**: HPCShell - An HPC-optimized shell for scientific computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5dda7",
   "metadata": {},
   "source": [
    "## Section 1: Resource Allocation and Job Configuration\n",
    "\n",
    "### Key Concepts\n",
    "Resource allocation is the foundation of HPC job management. It ensures that:\n",
    "1. **Isolation**: Jobs don't interfere with each other\n",
    "2. **Fairness**: Resources are distributed equitably\n",
    "3. **Efficiency**: System resources are fully utilized\n",
    "4. **Predictability**: Job performance is consistent\n",
    "\n",
    "### Resource Types in HPCShell\n",
    "- **CPU Cores**: Individual processor cores allocated to jobs\n",
    "- **Memory**: RAM allocated with virtual memory limits\n",
    "- **GPUs**: Graphics processors for compute acceleration\n",
    "- **Time Limits**: Maximum runtime before auto-termination\n",
    "- **I/O Bandwidth**: Disk and network bandwidth quotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ca6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESOURCE ALLOCATION CONFIGURATIONS\n",
      "======================================================================\n",
      "LIGHT               : Cores: 1 | Memory: 2.0 GB | Time Limit: 24.0h\n",
      "STANDARD            : Cores: 4 | Memory: 8.0 GB | Time Limit: 24.0h\n",
      "COMPUTE             : Cores: 16 | Memory: 32.0 GB | GPUs: 1 | Time Limit: 24.0h\n",
      "GPU_INTENSIVE       : Cores: 32 | Memory: 128.0 GB | GPUs: 4 | Time Limit: 48.0h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "\n",
    "# Add HPC shell to path\n",
    "project_root = Path('c:/Users/Liver/OneDrive/Desktop/HPC_shell')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Example 1: Resource Configuration Class\n",
    "@dataclass\n",
    "class ResourceConfig:\n",
    "    \"\"\"Configuration for job resource allocation\"\"\"\n",
    "    cores: int = 1\n",
    "    memory_gb: float = 1.0\n",
    "    gpu_count: int = 0\n",
    "    time_limit_hours: float = 24.0\n",
    "    disk_quota_gb: float = 100.0\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for storage\"\"\"\n",
    "        return asdict(self)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dict(data: Dict) -> 'ResourceConfig':\n",
    "        \"\"\"Create from dictionary\"\"\"\n",
    "        return ResourceConfig(**data)\n",
    "    \n",
    "    def __str__(self):\n",
    "        info = [f\"Cores: {self.cores}\"]\n",
    "        info.append(f\"Memory: {self.memory_gb:.1f} GB\")\n",
    "        if self.gpu_count > 0:\n",
    "            info.append(f\"GPUs: {self.gpu_count}\")\n",
    "        info.append(f\"Time Limit: {self.time_limit_hours:.1f}h\")\n",
    "        return \" | \".join(info)\n",
    "\n",
    "# Example configurations\n",
    "configs = {\n",
    "    'light': ResourceConfig(cores=1, memory_gb=2.0, gpu_count=0),\n",
    "    'standard': ResourceConfig(cores=4, memory_gb=8.0, gpu_count=0),\n",
    "    'compute': ResourceConfig(cores=16, memory_gb=32.0, gpu_count=1),\n",
    "    'gpu_intensive': ResourceConfig(cores=32, memory_gb=128.0, gpu_count=4, time_limit_hours=48.0),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESOURCE ALLOCATION CONFIGURATIONS\")\n",
    "print(\"=\" * 70)\n",
    "for name, config in configs.items():\n",
    "    print(f\"{name.upper():20s}: {config}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc6022",
   "metadata": {},
   "source": [
    "## Section 2: Job Scheduling and Priority Management\n",
    "\n",
    "### Scheduling Algorithms\n",
    "- **FIFO (First In, First Out)**: Simple queue-based scheduling\n",
    "- **Priority Queues**: Jobs scheduled by priority level\n",
    "- **Fair-Share**: Allocate resources based on user/group shares\n",
    "- **Backfill**: Fill gaps in schedule with smaller jobs\n",
    "- **Gang Scheduling**: Co-schedule related jobs together\n",
    "\n",
    "### Priority Levels\n",
    "| Level | Value | Use Case |\n",
    "|-------|-------|----------|\n",
    "| CRITICAL | 0 | System maintenance, emergency tasks |\n",
    "| HIGH | 1 | Interactive jobs, time-sensitive work |\n",
    "| NORMAL | 2 | Regular batch jobs (default) |\n",
    "| LOW | 3 | Background analysis, opportunistic work |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76708590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAIR-SHARE SCHEDULING EXAMPLE\n",
      "======================================================================\n",
      "User            CPU Used     Memory Used  Fair-Share\n",
      "----------------------------------------------------------------------\n",
      "alice           20           100          83.00%\n",
      "bob             60           200          52.00%\n",
      "charlie         10           50           91.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Advanced Scheduling with Priority and Fair-Share\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "\n",
    "class Priority(Enum):\n",
    "    CRITICAL = 0\n",
    "    HIGH = 1\n",
    "    NORMAL = 2\n",
    "    LOW = 3\n",
    "\n",
    "@dataclass\n",
    "class SchedulingPolicy:\n",
    "    \"\"\"Scheduling policy configuration\"\"\"\n",
    "    algorithm: str  # 'fifo', 'priority', 'fairshare', 'backfill'\n",
    "    priority_boost: float = 1.0  # Boost for older jobs\n",
    "    max_backfill_jobs: int = 10\n",
    "    fairshare_weight_cpu: float = 0.7\n",
    "    fairshare_weight_memory: float = 0.3\n",
    "    \n",
    "class FairShareScheduler:\n",
    "    \"\"\"Implement fair-share scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, policy: SchedulingPolicy):\n",
    "        self.policy = policy\n",
    "        self.user_allocation = defaultdict(lambda: {'cpu_used': 0, 'memory_used': 0})\n",
    "        self.user_quota = defaultdict(lambda: {'cpu': 100, 'memory': 1000})\n",
    "    \n",
    "    def calculate_fair_share(self, user: str) -> float:\n",
    "        \"\"\"Calculate fair-share score for user (0-1, higher is better)\"\"\"\n",
    "        cpu_used = self.user_allocation[user]['cpu_used']\n",
    "        cpu_quota = self.user_quota[user]['cpu']\n",
    "        cpu_ratio = cpu_used / cpu_quota if cpu_quota > 0 else 0\n",
    "        \n",
    "        memory_used = self.user_allocation[user]['memory_used']\n",
    "        memory_quota = self.user_quota[user]['memory']\n",
    "        memory_ratio = memory_used / memory_quota if memory_quota > 0 else 0\n",
    "        \n",
    "        # Weighted combination\n",
    "        fair_share = (\n",
    "            (1 - cpu_ratio) * self.policy.fairshare_weight_cpu +\n",
    "            (1 - memory_ratio) * self.policy.fairshare_weight_memory\n",
    "        )\n",
    "        return max(0.0, min(1.0, fair_share))\n",
    "    \n",
    "    def schedule_job(self, jobs: List[Dict], available_resources: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Select next job to schedule based on fair-share\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for job in jobs:\n",
    "            if job['status'] == 'queued':\n",
    "                user = job.get('user', 'unknown')\n",
    "                fair_share = self.calculate_fair_share(user)\n",
    "                candidates.append((fair_share, job))\n",
    "        \n",
    "        if not candidates:\n",
    "            return None\n",
    "        \n",
    "        # Sort by fair-share (descending)\n",
    "        candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = candidates[0][1]\n",
    "        return selected\n",
    "\n",
    "# Example simulation\n",
    "scheduler = FairShareScheduler(SchedulingPolicy(algorithm='fairshare'))\n",
    "\n",
    "# Simulate user allocations\n",
    "scheduler.user_allocation['alice'] = {'cpu_used': 20, 'memory_used': 100}\n",
    "scheduler.user_allocation['bob'] = {'cpu_used': 60, 'memory_used': 200}\n",
    "scheduler.user_allocation['charlie'] = {'cpu_used': 10, 'memory_used': 50}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FAIR-SHARE SCHEDULING EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'User':<15} {'CPU Used':<12} {'Memory Used':<12} {'Fair-Share':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for user in ['alice', 'bob', 'charlie']:\n",
    "    fair_share = scheduler.calculate_fair_share(user)\n",
    "    cpu = scheduler.user_allocation[user]['cpu_used']\n",
    "    mem = scheduler.user_allocation[user]['memory_used']\n",
    "    print(f\"{user:<15} {cpu:<12} {mem:<12} {fair_share:.2%}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab29f0d",
   "metadata": {},
   "source": [
    "## Section 3: Job Dependencies and Arrays\n",
    "\n",
    "### Job Dependencies\n",
    "Jobs can depend on other jobs completing successfully before they start:\n",
    "- **afterok**: Start after job X completes successfully\n",
    "- **afternotok**: Start after job X fails\n",
    "- **aftercorr**: Start after job X completes\n",
    "- **singleton**: Wait for all previous jobs with same name\n",
    "\n",
    "### Job Arrays\n",
    "Run the same job with different parameters:\n",
    "- Useful for parameter sweeps\n",
    "- Thousands of similar jobs from single submission\n",
    "- Automatic array index variable substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c99cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "JOB DEPENDENCY GRAPH (DAG)\n",
      "======================================================================\n",
      "\n",
      "Workflow: preprocessing -> simulation -> postprocessing -> analysis\n",
      "\n",
      "Job 1: preprocess_data           ✓ CAN RUN\n",
      "Job 2: run_simulation            ✓ CAN RUN\n",
      "Job 3: postprocess               ⏳ WAITING\n",
      "Job 4: analyze_results           ⏳ WAITING\n",
      "\n",
      "======================================================================\n",
      "JOB ARRAY EXAMPLE\n",
      "======================================================================\n",
      "\n",
      "Job array: process_data[1-10] processes 10 input files\n",
      "\n",
      "Array Index     Input File                Command\n",
      "-----------------------------------------------------------------\n",
      "1               data/input_01.txt         python process.py data/input_01.txt > data/output_01.txt\n",
      "2               data/input_02.txt         python process.py data/input_02.txt > data/output_02.txt\n",
      "3               data/input_03.txt         python process.py data/input_03.txt > data/output_03.txt\n",
      "4               data/input_04.txt         python process.py data/input_04.txt > data/output_04.txt\n",
      "5               data/input_05.txt         python process.py data/input_05.txt > data/output_05.txt\n",
      "6               data/input_06.txt         python process.py data/input_06.txt > data/output_06.txt\n",
      "7               data/input_07.txt         python process.py data/input_07.txt > data/output_07.txt\n",
      "8               data/input_08.txt         python process.py data/input_08.txt > data/output_08.txt\n",
      "9               data/input_09.txt         python process.py data/input_09.txt > data/output_09.txt\n",
      "10              data/input_10.txt         python process.py data/input_10.txt > data/output_10.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Job Dependencies and Arrays\n",
    "from typing import Set, Tuple\n",
    "\n",
    "@dataclass\n",
    "class JobDependency:\n",
    "    \"\"\"Represents a job dependency\"\"\"\n",
    "    job_id: int\n",
    "    dependency_type: str  # 'afterok', 'afternotok', 'aftercorr'\n",
    "    parent_job_id: int\n",
    "\n",
    "class JobDependencyGraph:\n",
    "    \"\"\"Manages job dependencies as DAG\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dependencies = {}  # job_id -> list of dependencies\n",
    "        self.dependents = {}    # job_id -> list of dependent jobs\n",
    "    \n",
    "    def add_dependency(self, job_id: int, parent_id: int, dep_type: str = 'afterok'):\n",
    "        \"\"\"Add dependency: job_id depends on parent_id\"\"\"\n",
    "        if job_id not in self.dependencies:\n",
    "            self.dependencies[job_id] = []\n",
    "        self.dependencies[job_id].append({'parent': parent_id, 'type': dep_type})\n",
    "        \n",
    "        if parent_id not in self.dependents:\n",
    "            self.dependents[parent_id] = []\n",
    "        self.dependents[parent_id].append(job_id)\n",
    "    \n",
    "    def can_run(self, job_id: int, completed_jobs: Set[int], failed_jobs: Set[int]) -> bool:\n",
    "        \"\"\"Check if job can run based on dependencies\"\"\"\n",
    "        if job_id not in self.dependencies:\n",
    "            return True\n",
    "        \n",
    "        for dep in self.dependencies[job_id]:\n",
    "            parent_id = dep['parent']\n",
    "            dep_type = dep['type']\n",
    "            \n",
    "            if dep_type == 'afterok':\n",
    "                # Parent must complete successfully\n",
    "                if parent_id not in completed_jobs:\n",
    "                    return False\n",
    "                if parent_id in failed_jobs:\n",
    "                    return False\n",
    "            elif dep_type == 'afternotok':\n",
    "                # Parent must fail\n",
    "                if parent_id not in failed_jobs:\n",
    "                    return False\n",
    "            elif dep_type == 'aftercorr':\n",
    "                # Parent must complete (success or fail)\n",
    "                if parent_id not in completed_jobs and parent_id not in failed_jobs:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Example DAG workflow\n",
    "dag = JobDependencyGraph()\n",
    "\n",
    "# Workflow: preprocessing -> simulation -> post-processing -> analysis\n",
    "jobs = {\n",
    "    1: {'name': 'preprocess_data', 'status': 'completed'},\n",
    "    2: {'name': 'run_simulation', 'status': 'pending'},\n",
    "    3: {'name': 'postprocess', 'status': 'pending'},\n",
    "    4: {'name': 'analyze_results', 'status': 'pending'},\n",
    "}\n",
    "\n",
    "# Add dependencies\n",
    "dag.add_dependency(2, 1, 'afterok')      # simulation after preprocessing\n",
    "dag.add_dependency(3, 2, 'afterok')      # postprocess after simulation\n",
    "dag.add_dependency(4, 3, 'afterok')      # analysis after postprocessing\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"JOB DEPENDENCY GRAPH (DAG)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWorkflow: preprocessing -> simulation -> postprocessing -> analysis\\n\")\n",
    "\n",
    "completed = {1}\n",
    "failed = set()\n",
    "\n",
    "for job_id in range(1, 5):\n",
    "    can_run = dag.can_run(job_id, completed, failed)\n",
    "    status = \"✓ CAN RUN\" if can_run else \"⏳ WAITING\"\n",
    "    print(f\"Job {job_id}: {jobs[job_id]['name']:<25} {status}\")\n",
    "\n",
    "# Example job array\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"JOB ARRAY EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nJob array: process_data[1-10] processes 10 input files\\n\")\n",
    "print(f\"{'Array Index':<15} {'Input File':<25} {'Command'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    input_file = f\"data/input_{i:02d}.txt\"\n",
    "    output_file = f\"data/output_{i:02d}.txt\"\n",
    "    cmd = f\"python process.py {input_file} > {output_file}\"\n",
    "    print(f\"{i:<15} {input_file:<25} {cmd}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8c416",
   "metadata": {},
   "source": [
    "## Section 4: Performance Monitoring and Metrics\n",
    "\n",
    "### Key Metrics\n",
    "1. **CPU Metrics**\n",
    "   - Utilization per core (%)\n",
    "   - Context switches per second\n",
    "   - Cache hit rates\n",
    "\n",
    "2. **Memory Metrics**\n",
    "   - Resident Set Size (RSS)\n",
    "   - Virtual Memory Size (VMS)\n",
    "   - Page fault rates\n",
    "\n",
    "3. **I/O Metrics**\n",
    "   - Read/write bandwidth (MB/s)\n",
    "   - IOPS (operations per second)\n",
    "   - Latency (ms)\n",
    "\n",
    "4. **System Metrics**\n",
    "   - Load average\n",
    "   - Temperature\n",
    "   - Power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea45a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PERFORMANCE MONITORING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "CPU Usage Statistics (last 10 samples):\n",
      "  Mean:   26.8%\n",
      "  Max:    85.0%\n",
      "  Min:    17.1%\n",
      "  StdDev: 21.9%\n",
      "\n",
      "Memory Usage Statistics:\n",
      "  Mean:   515.2 MB\n",
      "  Max:    523.2 MB\n",
      "  Min:    504.4 MB\n",
      "\n",
      "Anomalies Detected: 1\n",
      "  - cpu_spike: 85.0% (threshold: 70.6%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Performance Monitoring System\n",
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Container for performance metrics\"\"\"\n",
    "    timestamp: float\n",
    "    cpu_percent: float\n",
    "    memory_mb: float\n",
    "    io_read_mb: float\n",
    "    io_write_mb: float\n",
    "    network_mb: float = 0.0\n",
    "    gpu_util: float = 0.0\n",
    "    temp_c: float = 0.0\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor and analyze job performance\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 60):\n",
    "        self.window_size = window_size\n",
    "        self.metrics = deque(maxlen=window_size)\n",
    "        self.anomalies = []\n",
    "    \n",
    "    def record_metrics(self, metrics: PerformanceMetrics):\n",
    "        \"\"\"Record performance snapshot\"\"\"\n",
    "        self.metrics.append(metrics)\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Calculate statistics over window\"\"\"\n",
    "        if not self.metrics:\n",
    "            return {}\n",
    "        \n",
    "        cpu_values = [m.cpu_percent for m in self.metrics]\n",
    "        mem_values = [m.memory_mb for m in self.metrics]\n",
    "        \n",
    "        return {\n",
    "            'cpu': {\n",
    "                'mean': statistics.mean(cpu_values),\n",
    "                'max': max(cpu_values),\n",
    "                'min': min(cpu_values),\n",
    "                'stdev': statistics.stdev(cpu_values) if len(cpu_values) > 1 else 0,\n",
    "            },\n",
    "            'memory': {\n",
    "                'mean': statistics.mean(mem_values),\n",
    "                'max': max(mem_values),\n",
    "                'min': min(mem_values),\n",
    "            },\n",
    "            'sample_count': len(self.metrics)\n",
    "        }\n",
    "    \n",
    "    def detect_anomalies(self) -> List[Dict]:\n",
    "        \"\"\"Detect performance anomalies\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        anomalies = []\n",
    "        \n",
    "        if not stats or len(self.metrics) < 5:\n",
    "            return anomalies\n",
    "        \n",
    "        cpu_mean = stats['cpu']['mean']\n",
    "        cpu_stdev = stats['cpu']['stdev']\n",
    "        \n",
    "        # Check for unusual CPU spikes (> 2 sigma)\n",
    "        threshold = cpu_mean + (2 * cpu_stdev)\n",
    "        \n",
    "        for i, metric in enumerate(list(self.metrics)[-5:]):\n",
    "            if metric.cpu_percent > threshold:\n",
    "                anomalies.append({\n",
    "                    'type': 'cpu_spike',\n",
    "                    'value': metric.cpu_percent,\n",
    "                    'threshold': threshold,\n",
    "                    'time': metric.timestamp\n",
    "                })\n",
    "        \n",
    "        return anomalies\n",
    "\n",
    "# Simulate performance data\n",
    "monitor = PerformanceMonitor(window_size=10)\n",
    "\n",
    "import random\n",
    "current_time = time.time()\n",
    "\n",
    "# Normal operation followed by spike\n",
    "for i in range(8):\n",
    "    metrics = PerformanceMetrics(\n",
    "        timestamp=current_time + i,\n",
    "        cpu_percent=20 + random.gauss(0, 2),\n",
    "        memory_mb=512 + random.gauss(0, 10),\n",
    "        io_read_mb=5.0 + random.gauss(0, 0.5),\n",
    "        io_write_mb=3.0 + random.gauss(0, 0.3),\n",
    "    )\n",
    "    monitor.record_metrics(metrics)\n",
    "\n",
    "# Add anomaly\n",
    "anomaly_metrics = PerformanceMetrics(\n",
    "    timestamp=current_time + 8,\n",
    "    cpu_percent=85.0,  # CPU spike\n",
    "    memory_mb=520,\n",
    "    io_read_mb=12.0,\n",
    "    io_write_mb=15.0,\n",
    ")\n",
    "monitor.record_metrics(anomaly_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE MONITORING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "stats = monitor.get_statistics()\n",
    "print(f\"\\nCPU Usage Statistics (last {monitor.window_size} samples):\")\n",
    "print(f\"  Mean:   {stats['cpu']['mean']:.1f}%\")\n",
    "print(f\"  Max:    {stats['cpu']['max']:.1f}%\")\n",
    "print(f\"  Min:    {stats['cpu']['min']:.1f}%\")\n",
    "print(f\"  StdDev: {stats['cpu']['stdev']:.1f}%\")\n",
    "\n",
    "print(f\"\\nMemory Usage Statistics:\")\n",
    "print(f\"  Mean:   {stats['memory']['mean']:.1f} MB\")\n",
    "print(f\"  Max:    {stats['memory']['max']:.1f} MB\")\n",
    "print(f\"  Min:    {stats['memory']['min']:.1f} MB\")\n",
    "\n",
    "anomalies = monitor.detect_anomalies()\n",
    "print(f\"\\nAnomalies Detected: {len(anomalies)}\")\n",
    "for anomaly in anomalies:\n",
    "    print(f\"  - {anomaly['type']}: {anomaly['value']:.1f}% (threshold: {anomaly['threshold']:.1f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808eef2",
   "metadata": {},
   "source": [
    "## Section 5: Benchmarking CPU, Memory, and I/O\n",
    "\n",
    "### CPU Benchmarking Metrics\n",
    "- **FLOPS**: Floating-point operations per second\n",
    "- **Integer Operations**: Fixed-point arithmetic throughput\n",
    "- **Matrix Multiplication**: Cache efficiency indicator\n",
    "- **Memory Bandwidth**: Memory subsystem throughput\n",
    "\n",
    "### Memory Benchmarking\n",
    "- **Sequential Access**: Linear memory bandwidth\n",
    "- **Random Access**: Cache effects on latency\n",
    "- **Strided Access**: NUMA locality effects\n",
    "\n",
    "### I/O Benchmarking\n",
    "- **Sequential Read/Write**: Peak throughput\n",
    "- **Random I/O**: IOPS performance\n",
    "- **Mixed Workload**: Real-world patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9070f6e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'field' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example 5: Comprehensive Benchmarking\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mBenchmarkResult\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Results from a single benchmark run\"\"\"\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mBenchmarkResult\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m score: \u001b[38;5;28mfloat\u001b[39m\n\u001b[0;32m      9\u001b[0m unit: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m---> 10\u001b[0m timestamp: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mfield\u001b[49m(default_factory\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'field' is not defined"
     ]
    }
   ],
   "source": [
    "# Example 5: Comprehensive Benchmarking\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Results from a single benchmark run\"\"\"\n",
    "    name: str\n",
    "    score: float\n",
    "    unit: str\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "\n",
    "class BenchmarkSuite:\n",
    "    \"\"\"Suite of system benchmarks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def cpu_flops_benchmark(self, duration: float = 1.0) -> BenchmarkResult:\n",
    "        \"\"\"Benchmark floating-point operations\"\"\"\n",
    "        iterations = 0\n",
    "        a, b, c = 1.5, 2.5, 3.5\n",
    "        \n",
    "        start = time.time()\n",
    "        end_time = start + duration\n",
    "        \n",
    "        while time.time() < end_time:\n",
    "            for _ in range(1000):\n",
    "                a = a * b + c\n",
    "                b = b * c + a\n",
    "                c = c * a + b\n",
    "            iterations += 1000\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        flops = (iterations * 6) / elapsed  # 6 FLOPs per iteration\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            name='CPU FLOPS',\n",
    "            score=flops / 1e9,  # Convert to GFLOPs\n",
    "            unit='GFLOPS'\n",
    "        )\n",
    "    \n",
    "    def memory_bandwidth_benchmark(self, size_mb: int = 100) -> BenchmarkResult:\n",
    "        \"\"\"Benchmark memory bandwidth\"\"\"\n",
    "        array = bytearray(size_mb * 1024 * 1024)\n",
    "        \n",
    "        start = time.time()\n",
    "        # Simulate sequential memory access\n",
    "        sum_val = 0\n",
    "        for i in range(0, len(array), 64):  # 64-byte stride (cache line)\n",
    "            sum_val += array[i]\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        bandwidth = (size_mb * 1024 * 1024) / (elapsed * 1e9)\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            name='Memory Bandwidth',\n",
    "            score=bandwidth,\n",
    "            unit='GB/s'\n",
    "        )\n",
    "    \n",
    "    def io_sequential_benchmark(self, file_size_mb: int = 100) -> Dict:\n",
    "        \"\"\"Benchmark sequential I/O\"\"\"\n",
    "        import tempfile\n",
    "        \n",
    "        # Write test\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "            temp_file = f.name\n",
    "            start = time.time()\n",
    "            bytes_written = 0\n",
    "            while bytes_written < file_size_mb * 1024 * 1024:\n",
    "                chunk = b'x' * (1024 * 1024)  # 1MB chunks\n",
    "                f.write(chunk)\n",
    "                bytes_written += len(chunk)\n",
    "            write_time = time.time() - start\n",
    "        \n",
    "        # Read test\n",
    "        start = time.time()\n",
    "        with open(temp_file, 'rb') as f:\n",
    "            while f.read(1024 * 1024):\n",
    "                pass\n",
    "        read_time = time.time() - start\n",
    "        \n",
    "        # Cleanup\n",
    "        import os\n",
    "        os.unlink(temp_file)\n",
    "        \n",
    "        return {\n",
    "            'read_bandwidth': (file_size_mb * 1024 * 1024) / (read_time * 1e9),\n",
    "            'write_bandwidth': (file_size_mb * 1024 * 1024) / (write_time * 1e9),\n",
    "            'unit': 'GB/s'\n",
    "        }\n",
    "\n",
    "# Run benchmarks\n",
    "suite = BenchmarkSuite()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SYSTEM BENCHMARK SUITE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# CPU benchmark\n",
    "cpu_result = suite.cpu_flops_benchmark(duration=0.5)\n",
    "print(f\"\\nCPU Performance: {cpu_result.score:.2f} {cpu_result.unit}\")\n",
    "\n",
    "# Memory benchmark\n",
    "mem_result = suite.memory_bandwidth_benchmark(size_mb=50)\n",
    "print(f\"Memory Bandwidth: {mem_result.score:.2f} {mem_result.unit}\")\n",
    "\n",
    "# I/O benchmark (skip for notebooks to save time)\n",
    "print(f\"I/O Sequential: (simulated) Read: 500.0 MB/s, Write: 450.0 MB/s\")\n",
    "\n",
    "# Overall score calculation\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Benchmark Summary\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "scores = {\n",
    "    'CPU (normalized)': cpu_result.score / 10.0,  # Normalize to 0-10\n",
    "    'Memory (normalized)': mem_result.score / 20.0,  # Normalize\n",
    "}\n",
    "\n",
    "overall = sum(scores.values()) / len(scores)\n",
    "print(f\"Overall Score: {overall:.1f}/10\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39251b6",
   "metadata": {},
   "source": [
    "## Section 6: Parallel Execution Strategies\n",
    "\n",
    "### Parallelization Models\n",
    "1. **Data Parallelism**: Process different data chunks in parallel\n",
    "2. **Task Parallelism**: Run different tasks simultaneously\n",
    "3. **Pipeline Parallelism**: Chain operations across cores\n",
    "4. **SIMD**: Single instruction, multiple data operations\n",
    "5. **MPI (Message Passing Interface)**: Distributed memory parallelism\n",
    "\n",
    "### GNU Parallel-like Syntax\n",
    "```bash\n",
    "# Process multiple files\n",
    "parallel process_file {} ::: file1 file2 file3\n",
    "\n",
    "# Use multiple cores\n",
    "parallel --jobs 4 command {} ::: input1 input2 input3\n",
    "\n",
    "# Chain operations\n",
    "parallel 'preprocess {} | analyze | generate_report' ::: data_*.csv\n",
    "```\n",
    "\n",
    "### Load Balancing\n",
    "- **Static**: Pre-assign work to cores\n",
    "- **Dynamic**: Distribute work as cores become available\n",
    "- **Guided**: Gradually reduce chunk size as load balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PARALLEL EXECUTION EXAMPLE\n",
      "======================================================================\n",
      "\n",
      "Processing 8 files with 4 workers\n",
      "Files: ['file_0.txt', 'file_1.txt', 'file_2.txt']... (showing first 3 of 8)\n",
      "\n",
      "Sequential execution:\n",
      "  Time: 0.30s\n",
      "\n",
      "Parallel execution (4 workers):\n",
      "  Time: 0.30s\n",
      "\n",
      "Parallel execution (4 workers):\n"
     ]
    }
   ],
   "source": [
    "# Example 6: Parallel Execution Engine\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "class ParallelExecutor:\n",
    "    \"\"\"Execute tasks in parallel with load balancing\"\"\"\n",
    "    \n",
    "    def __init__(self, num_workers: int = None):\n",
    "        self.num_workers = num_workers or cpu_count()\n",
    "        self.completed = 0\n",
    "        self.failed = 0\n",
    "    \n",
    "    def map_function(self, func: Callable, data: Iterable) -> List:\n",
    "        \"\"\"Map function over data using multiprocessing\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            with Pool(processes=self.num_workers) as pool:\n",
    "                results = pool.map(func, data)\n",
    "            self.completed = len(results)\n",
    "        except Exception as e:\n",
    "            self.failed += 1\n",
    "            print(f\"Error in parallel execution: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def dynamic_load_balancing(self, tasks: List[Dict], num_cores: int) -> List[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Distribute tasks using dynamic load balancing.\n",
    "        Returns list of task groups for each core.\n",
    "        \"\"\"\n",
    "        work_queues = [[] for _ in range(num_cores)]\n",
    "        task_times = {task['id']: task.get('estimated_time', 1.0) for task in tasks}\n",
    "        \n",
    "        # Greedy assignment: assign to least loaded core\n",
    "        for task in sorted(tasks, key=lambda t: t_times[t['id']], reverse=True):\n",
    "            min_idx = min(range(num_cores), key=lambda i: sum(task_times[t['id']] for t in work_queues[i]))\n",
    "            work_queues[min_idx].append(task)\n",
    "        \n",
    "        return work_queues\n",
    "\n",
    "# Example: Processing files in parallel\n",
    "def process_file(filename):\n",
    "    \"\"\"Simulate file processing\"\"\"\n",
    "    time.sleep(0.1)\n",
    "    return f\"Processed {filename}\"\n",
    "\n",
    "# Simulate parallel execution\n",
    "executor = ParallelExecutor(num_workers=4)\n",
    "\n",
    "test_files = [f\"file_{i}.txt\" for i in range(8)]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PARALLEL EXECUTION EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nProcessing {len(test_files)} files with {executor.num_workers} workers\")\n",
    "print(f\"Files: {test_files[:3]}... (showing first 3 of {len(test_files)})\")\n",
    "\n",
    "# Sequential execution\n",
    "print(\"\\nSequential execution:\")\n",
    "start = time.time()\n",
    "seq_results = [process_file(f) for f in test_files[:3]]\n",
    "seq_time = time.time() - start\n",
    "print(f\"  Time: {seq_time:.2f}s\")\n",
    "\n",
    "# Parallel execution\n",
    "print(f\"\\nParallel execution ({executor.num_workers} workers):\")\n",
    "start = time.time()\n",
    "par_results = executor.map_function(process_file, test_files)\n",
    "par_time = time.time() - start\n",
    "print(f\"  Time: {par_time:.2f}s\")\n",
    "print(f\"  Speedup: {seq_time / par_time:.1f}x\")\n",
    "\n",
    "# Load balancing example\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Dynamic Load Balancing\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "tasks = [\n",
    "    {'id': 1, 'name': 'simulation_1', 'estimated_time': 10.0},\n",
    "    {'id': 2, 'name': 'analysis_1', 'estimated_time': 5.0},\n",
    "    {'id': 3, 'name': 'simulation_2', 'estimated_time': 15.0},\n",
    "    {'id': 4, 'name': 'analysis_2', 'estimated_time': 3.0},\n",
    "    {'id': 5, 'name': 'simulation_3', 'estimated_time': 8.0},\n",
    "]\n",
    "\n",
    "queues = executor.dynamic_load_balancing(tasks, num_cores=2)\n",
    "\n",
    "for core_idx, queue in enumerate(queues):\n",
    "    total_time = sum(t['estimated_time'] for t in queue)\n",
    "    print(f\"\\nCore {core_idx}: {total_time:.1f}s\")\n",
    "    for task in queue:\n",
    "        print(f\"  - {task['name']}: {task['estimated_time']:.1f}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b8440",
   "metadata": {},
   "source": [
    "## Section 7: Machine Learning-Based Optimization\n",
    "\n",
    "### ML Applications in HPC\n",
    "1. **Runtime Prediction**: Predict job execution time based on characteristics\n",
    "2. **Resource Recommendation**: Suggest optimal core/memory allocation\n",
    "3. **Anomaly Detection**: Identify hung or problematic jobs\n",
    "4. **Workload Pattern Analysis**: Classify and predict workload patterns\n",
    "5. **Smart Job Placement**: Optimize job placement on nodes\n",
    "\n",
    "### Feature Engineering for ML\n",
    "- Job characteristics: command type, data size, complexity\n",
    "- Historical performance: past execution times, resource usage\n",
    "- System state: current load, available resources\n",
    "- User patterns: typical job size, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b709480",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example 7: ML-Based Job Runtime Prediction\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mRuntimePredictor\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Predict job runtime using historical data\"\"\"\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m, in \u001b[0;36mRuntimePredictor\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mgb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[key]\u001b[38;5;241m.\u001b[39mappend(runtime_sec)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_runtime\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_type: \u001b[38;5;28mstr\u001b[39m, cores: \u001b[38;5;28mint\u001b[39m, memory_gb: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mDict\u001b[49m:\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict runtime for job configuration\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mgb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Example 7: ML-Based Job Runtime Prediction\n",
    "from collections import defaultdict\n",
    "\n",
    "class RuntimePredictor:\n",
    "    \"\"\"Predict job runtime using historical data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = defaultdict(list)  # job_type -> list of runtimes\n",
    "    \n",
    "    def record_job(self, job_type: str, cores: int, memory_gb: float, runtime_sec: float):\n",
    "        \"\"\"Record completed job for training\"\"\"\n",
    "        key = f\"{job_type}_{cores}c_{memory_gb}gb\"\n",
    "        self.history[key].append(runtime_sec)\n",
    "    \n",
    "    def predict_runtime(self, job_type: str, cores: int, memory_gb: float) -> Dict:\n",
    "        \"\"\"Predict runtime for job configuration\"\"\"\n",
    "        key = f\"{job_type}_{cores}c_{memory_gb}gb\"\n",
    "        \n",
    "        if key in self.history and len(self.history[key]) > 0:\n",
    "            times = self.history[key]\n",
    "            mean = statistics.mean(times)\n",
    "            stdev = statistics.stdev(times) if len(times) > 1 else 0\n",
    "            \n",
    "            return {\n",
    "                'predicted_time': mean,\n",
    "                'confidence': 0.9 if len(times) > 5 else 0.6,\n",
    "                'samples': len(times),\n",
    "                'std_dev': stdev,\n",
    "            }\n",
    "        else:\n",
    "            # No history, use defaults\n",
    "            return {\n",
    "                'predicted_time': 300.0,  # 5 minutes default\n",
    "                'confidence': 0.1,\n",
    "                'samples': 0,\n",
    "                'std_dev': None,\n",
    "            }\n",
    "    \n",
    "    def recommend_cores(self, job_type: str, target_time_sec: float = 300) -> int:\n",
    "        \"\"\"Recommend number of cores for job\"\"\"\n",
    "        # Collect all measurements for this job type\n",
    "        measurements = []\n",
    "        for key, times in self.history.items():\n",
    "            if key.startswith(job_type):\n",
    "                measurements.extend(times)\n",
    "        \n",
    "        if not measurements:\n",
    "            return 4  # Default recommendation\n",
    "        \n",
    "        # Estimate: doubling cores roughly halves runtime (Amdahl's law assumption)\n",
    "        avg_time = statistics.mean(measurements)\n",
    "        cores_needed = max(1, int(4 * (avg_time / target_time_sec)))\n",
    "        \n",
    "        return cores_needed\n",
    "\n",
    "# Simulate training data\n",
    "predictor = RuntimePredictor()\n",
    "\n",
    "# Record historical jobs\n",
    "training_data = [\n",
    "    ('simulation', 1, 2.0, 120),\n",
    "    ('simulation', 1, 2.0, 130),\n",
    "    ('simulation', 4, 4.0, 40),\n",
    "    ('simulation', 4, 4.0, 38),\n",
    "    ('simulation', 8, 8.0, 20),\n",
    "    ('analysis', 1, 2.0, 60),\n",
    "    ('analysis', 1, 2.0, 65),\n",
    "    ('analysis', 4, 4.0, 18),\n",
    "]\n",
    "\n",
    "for job_type, cores, memory, runtime in training_data:\n",
    "    predictor.record_job(job_type, cores, memory, runtime)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ML-BASED JOB RUNTIME PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test predictions\n",
    "test_cases = [\n",
    "    ('simulation', 2, 4.0),\n",
    "    ('simulation', 8, 8.0),\n",
    "    ('analysis', 4, 4.0),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Job Type':<15} {'Cores':<8} {'Memory':<10} {'Predicted':<12} {'Confidence':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for job_type, cores, memory in test_cases:\n",
    "    pred = predictor.predict_runtime(job_type, cores, memory)\n",
    "    print(f\"{job_type:<15} {cores:<8} {memory:.1f} GB{pred['predicted_time']:<11.0f}s {pred['confidence']:.0%}\")\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Resource Recommendations\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for job_type in ['simulation', 'analysis']:\n",
    "    recommended_cores = predictor.recommend_cores(job_type, target_time_sec=60)\n",
    "    print(f\"{job_type}: Recommend {recommended_cores} cores for ~60 second runtime\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbee81",
   "metadata": {},
   "source": [
    "## Section 8: Advanced Scheduling Algorithms\n",
    "\n",
    "### Gang Scheduling (Co-scheduling)\n",
    "Schedule related jobs together to:\n",
    "- Minimize synchronization overhead\n",
    "- Improve cache locality\n",
    "- Reduce context switching\n",
    "\n",
    "### Backfill Scheduling\n",
    "- Run smaller jobs in gaps while waiting for large job to start\n",
    "- Improves system utilization\n",
    "- Maintains fairness guarantees\n",
    "\n",
    "### Preemption\n",
    "- Temporarily suspend low-priority jobs\n",
    "- Resume when resources available\n",
    "- Requires checkpointing support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4fe661",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example 8: Advanced Scheduling Algorithms\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mJobSpec\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Job specification for scheduling\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     job_id: \u001b[38;5;28mint\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "# Example 8: Advanced Scheduling Algorithms\n",
    "@dataclass\n",
    "class JobSpec:\n",
    "    \"\"\"Job specification for scheduling\"\"\"\n",
    "    job_id: int\n",
    "    cores_needed: int\n",
    "    duration_sec: float\n",
    "    priority: int\n",
    "    submit_time: float\n",
    "\n",
    "class BackfillScheduler:\n",
    "    \"\"\"SLURM-style backfill scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, num_cores: int):\n",
    "        self.num_cores = num_cores\n",
    "        self.schedule = []\n",
    "        self.current_time = 0.0\n",
    "    \n",
    "    def can_fit(self, job: JobSpec, time_slot: Tuple[float, float]) -> bool:\n",
    "        \"\"\"Check if job can fit in time slot\"\"\"\n",
    "        start, end = time_slot\n",
    "        duration = min(job.duration_sec, end - start)\n",
    "        \n",
    "        # Check core availability in time window\n",
    "        cores_needed = job.cores_needed\n",
    "        available_cores = self._get_available_cores(start, end)\n",
    "        \n",
    "        return available_cores >= cores_needed\n",
    "    \n",
    "    def _get_available_cores(self, start_time: float, end_time: float) -> int:\n",
    "        \"\"\"Get available cores in time window\"\"\"\n",
    "        available = self.num_cores\n",
    "        \n",
    "        for scheduled_job, job_start, job_end in self.schedule:\n",
    "            if job_start < end_time and job_end > start_time:\n",
    "                # Job overlaps with time window\n",
    "                overlap_start = max(job_start, start_time)\n",
    "                overlap_end = min(job_end, end_time)\n",
    "                if overlap_end > overlap_start:\n",
    "                    available -= scheduled_job.cores_needed\n",
    "        \n",
    "        return max(0, available)\n",
    "    \n",
    "    def schedule_job(self, job: JobSpec) -> bool:\n",
    "        \"\"\"Try to schedule job using backfill\"\"\"\n",
    "        # First, try to schedule ASAP (no backfill)\n",
    "        earliest = self._find_earliest_start(job)\n",
    "        \n",
    "        if earliest is not None:\n",
    "            self.schedule.append((job, earliest, earliest + job.duration_sec))\n",
    "            return True\n",
    "        \n",
    "        # Try backfill in earlier gaps\n",
    "        for i in range(len(self.schedule)):\n",
    "            gap_start = self.schedule[i][2]  # End of previous job\n",
    "            gap_end = self.schedule[i+1][1] if i+1 < len(self.schedule) else float('inf')\n",
    "            \n",
    "            if self.can_fit(job, (gap_start, gap_end)):\n",
    "                self.schedule.append((job, gap_start, gap_start + job.duration_sec))\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _find_earliest_start(self, job: JobSpec) -> Optional[float]:\n",
    "        \"\"\"Find earliest time job can start\"\"\"\n",
    "        candidates = [0.0] + [end for _, _, end in self.schedule]\n",
    "        \n",
    "        for start_time in sorted(candidates):\n",
    "            available = self._get_available_cores(start_time, start_time + job.duration_sec)\n",
    "            if available >= job.cores_needed:\n",
    "                return start_time\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Simulate backfill scheduling\n",
    "scheduler = BackfillScheduler(num_cores=8)\n",
    "\n",
    "jobs = [\n",
    "    JobSpec(job_id=1, cores_needed=4, duration_sec=10, priority=2, submit_time=0),\n",
    "    JobSpec(job_id=2, cores_needed=8, duration_sec=20, priority=1, submit_time=1),\n",
    "    JobSpec(job_id=3, cores_needed=2, duration_sec=5, priority=2, submit_time=2),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BACKFILL SCHEDULING EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSystem: {scheduler.num_cores} cores available\\n\")\n",
    "\n",
    "for job in jobs:\n",
    "    scheduled = scheduler.schedule_job(job)\n",
    "    status = \"✓ Scheduled\" if scheduled else \"✗ Could not schedule\"\n",
    "    print(f\"Job {job.job_id}: {job.cores_needed} cores, {job.duration_sec}s duration -> {status}\")\n",
    "\n",
    "print(\"\\nSchedule Timeline:\")\n",
    "print(\"-\" * 70)\n",
    "for job, start, end in sorted(scheduler.schedule, key=lambda x: x[1]):\n",
    "    timeline = \"█\" * int(job.cores_needed) + \"░\" * (scheduler.num_cores - job.cores_needed)\n",
    "    print(f\"Job {job.job_id}: [{timeline}] t={start:.0f}-{end:.0f}s\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c7eba",
   "metadata": {},
   "source": [
    "## Section 9: Workflow Management with DAGs\n",
    "\n",
    "### DAG Workflow Features\n",
    "- **Directed Acyclic Graphs**: Define complex dependencies\n",
    "- **Workflow Templates**: Reusable workflow patterns\n",
    "- **Conditional Execution**: Branch on job results\n",
    "- **Retry Logic**: Automatic failure recovery\n",
    "- **Checkpointing**: Save/restore workflow state\n",
    "\n",
    "### Use Cases\n",
    "- **Scientific Pipelines**: Multi-stage data processing\n",
    "- **Parameter Sweeps**: Run variants of experiments\n",
    "- **Machine Learning**: Pipeline of preprocessing, training, evaluation\n",
    "- **Data Analysis**: Extract, transform, load workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1df662",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example 9: Workflow DAG Management\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict, deque\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mWorkflowDAG\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Manage Directed Acyclic Graph workflows\"\"\"\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mWorkflowDAG\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medges[parent_id]\u001b[38;5;241m.\u001b[39mappend(child_id)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_degree[child_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_executable_tasks\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tasks ready to run (no pending dependencies)\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     executable \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "# Example 9: Workflow DAG Management\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class WorkflowDAG:\n",
    "    \"\"\"Manage Directed Acyclic Graph workflows\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.edges = defaultdict(list)\n",
    "        self.in_degree = defaultdict(int)\n",
    "    \n",
    "    def add_task(self, task_id: str, command: str, retry_count: int = 3):\n",
    "        \"\"\"Add task to workflow\"\"\"\n",
    "        self.nodes[task_id] = {\n",
    "            'command': command,\n",
    "            'retry_count': retry_count,\n",
    "            'status': 'pending',\n",
    "            'output': None,\n",
    "        }\n",
    "    \n",
    "    def add_dependency(self, parent_id: str, child_id: str):\n",
    "        \"\"\"Add edge: parent -> child\"\"\"\n",
    "        self.edges[parent_id].append(child_id)\n",
    "        self.in_degree[child_id] += 1\n",
    "    \n",
    "    def get_executable_tasks(self) -> List[str]:\n",
    "        \"\"\"Get tasks ready to run (no pending dependencies)\"\"\"\n",
    "        executable = []\n",
    "        for task_id, task_data in self.nodes.items():\n",
    "            if task_data['status'] == 'pending' and self.in_degree[task_id] == 0:\n",
    "                executable.append(task_id)\n",
    "        return executable\n",
    "    \n",
    "    def mark_completed(self, task_id: str, output: str = None):\n",
    "        \"\"\"Mark task as complete and update dependencies\"\"\"\n",
    "        self.nodes[task_id]['status'] = 'completed'\n",
    "        self.nodes[task_id]['output'] = output\n",
    "        \n",
    "        # Update in-degree for dependent tasks\n",
    "        for child_id in self.edges[task_id]:\n",
    "            self.in_degree[child_id] -= 1\n",
    "    \n",
    "    def visualize(self) -> str:\n",
    "        \"\"\"ASCII visualization of workflow\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"Workflow DAG:\")\n",
    "        lines.append(\"-\" * 50)\n",
    "        \n",
    "        for task_id, task_data in self.nodes.items():\n",
    "            status_icon = \"✓\" if task_data['status'] == 'completed' else \"○\"\n",
    "            lines.append(f\"{status_icon} {task_id}: {task_data['command']}\")\n",
    "            \n",
    "            for child_id in self.edges[task_id]:\n",
    "                lines.append(f\"    └─> {child_id}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Create example ML pipeline workflow\n",
    "workflow = WorkflowDAG()\n",
    "\n",
    "# Add tasks\n",
    "workflow.add_task('load_data', 'python load_dataset.py')\n",
    "workflow.add_task('preprocess', 'python preprocess.py')\n",
    "workflow.add_task('train_model', 'python train.py')\n",
    "workflow.add_task('evaluate', 'python evaluate.py')\n",
    "workflow.add_task('generate_report', 'python report.py')\n",
    "\n",
    "# Add dependencies\n",
    "workflow.add_dependency('load_data', 'preprocess')\n",
    "workflow.add_dependency('preprocess', 'train_model')\n",
    "workflow.add_dependency('train_model', 'evaluate')\n",
    "workflow.add_dependency('evaluate', 'generate_report')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WORKFLOW DAG EXAMPLE: ML TRAINING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Simulate workflow execution\n",
    "executable_tasks = workflow.get_executable_tasks()\n",
    "print(f\"Initial executable tasks: {executable_tasks}\")\n",
    "\n",
    "# Execute workflow step by step\n",
    "step = 1\n",
    "while True:\n",
    "    executable = workflow.get_executable_tasks()\n",
    "    if not executable:\n",
    "        break\n",
    "    \n",
    "    task = executable[0]\n",
    "    print(f\"\\nStep {step}: Execute '{task}'\")\n",
    "    print(f\"  Command: {workflow.nodes[task]['command']}\")\n",
    "    workflow.mark_completed(task, f\"output_of_{task}\")\n",
    "    step += 1\n",
    "\n",
    "print(\"\\n\" + workflow.visualize())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425c88f",
   "metadata": {},
   "source": [
    "## Section 10: Security, Isolation, and Containers\n",
    "\n",
    "### User Isolation\n",
    "- **Namespace Isolation**: Users can't see other's jobs\n",
    "- **Resource Quotas**: Per-user resource limits\n",
    "- **File Permissions**: Access control on job files\n",
    "- **Audit Logging**: Track all user actions\n",
    "\n",
    "### Container Integration\n",
    "- **Docker Support**: Package jobs with dependencies\n",
    "- **Singularity**: HPC-friendly container runtime\n",
    "- **Container Orchestration**: Manage container lifecycle\n",
    "- **Reproducibility**: Identical environment everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d4c7e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     USER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m     GUEST \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mguest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUserQuota\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"User resource quota\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     user_id: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "# Example 10: Security and Resource Isolation\n",
    "from enum import Enum\n",
    "\n",
    "class UserRole(Enum):\n",
    "    ADMIN = 'admin'\n",
    "    USER = 'user'\n",
    "    GUEST = 'guest'\n",
    "\n",
    "@dataclass\n",
    "class UserQuota:\n",
    "    \"\"\"User resource quota\"\"\"\n",
    "    user_id: str\n",
    "    max_cores: int = 16\n",
    "    max_memory_gb: float = 64.0\n",
    "    max_jobs: int = 100\n",
    "    max_runtime_hours: float = 72.0\n",
    "    role: UserRole = UserRole.USER\n",
    "\n",
    "class SecurityManager:\n",
    "    \"\"\"Manage security and isolation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_quotas = {}\n",
    "        self.audit_log = []\n",
    "    \n",
    "    def register_user(self, user_id: str, quota: UserQuota):\n",
    "        \"\"\"Register user with quota\"\"\"\n",
    "        self.user_quotas[user_id] = quota\n",
    "        self._log_action('register', user_id, f\"User registered with quota\")\n",
    "    \n",
    "    def can_submit_job(self, user_id: str, cores: int, memory_gb: float, hours: float) -> Tuple[bool, str]:\n",
    "        \"\"\"Check if user can submit job\"\"\"\n",
    "        if user_id not in self.user_quotas:\n",
    "            return False, \"User not registered\"\n",
    "        \n",
    "        quota = self.user_quotas[user_id]\n",
    "        \n",
    "        if cores > quota.max_cores:\n",
    "            return False, f\"Exceeds core limit: {cores} > {quota.max_cores}\"\n",
    "        \n",
    "        if memory_gb > quota.max_memory_gb:\n",
    "            return False, f\"Exceeds memory limit: {memory_gb} > {quota.max_memory_gb}\"\n",
    "        \n",
    "        if hours > quota.max_runtime_hours:\n",
    "            return False, f\"Exceeds time limit: {hours} > {quota.max_runtime_hours}\"\n",
    "        \n",
    "        return True, \"OK\"\n",
    "    \n",
    "    def _log_action(self, action: str, user_id: str, details: str):\n",
    "        \"\"\"Log security action\"\"\"\n",
    "        self.audit_log.append({\n",
    "            'timestamp': time.time(),\n",
    "            'action': action,\n",
    "            'user': user_id,\n",
    "            'details': details\n",
    "        })\n",
    "\n",
    "# Example: Security setup\n",
    "sec_manager = SecurityManager()\n",
    "\n",
    "# Register users with quotas\n",
    "sec_manager.register_user('alice', UserQuota(\n",
    "    user_id='alice', max_cores=16, max_memory_gb=64, role=UserRole.USER\n",
    "))\n",
    "sec_manager.register_user('bob', UserQuota(\n",
    "    user_id='bob', max_cores=4, max_memory_gb=8, role=UserRole.GUEST\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECURITY AND RESOURCE ISOLATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test job submissions\n",
    "test_submissions = [\n",
    "    ('alice', 8, 32, 24, 'Large simulation'),\n",
    "    ('alice', 32, 128, 48, 'Very large job'),\n",
    "    ('bob', 2, 4, 4, 'Small job'),\n",
    "    ('bob', 8, 16, 24, 'Too large for quota'),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'User':<10} {'Cores':<8} {'Memory':<10} {'Hours':<8} {'Job':<20} {'Result':<20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for user_id, cores, memory, hours, job_name in test_submissions:\n",
    "    can_submit, reason = sec_manager.can_submit_job(user_id, cores, memory, hours)\n",
    "    result = \"✓ APPROVED\" if can_submit else f\"✗ {reason}\"\n",
    "    print(f\"{user_id:<10} {cores:<8} {memory:.0f} GB{hours:<8} {job_name:<20} {result:<20}\")\n",
    "\n",
    "# Example: Container configuration\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Container Support (Docker/Singularity)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "container_configs = {\n",
    "    'gpu_ml': {\n",
    "        'image': 'nvidia/cuda:11.2-runtime-ubuntu20.04',\n",
    "        'mounts': ['/data', '/scratch'],\n",
    "        'env': {'CUDA_VISIBLE_DEVICES': '0'},\n",
    "    },\n",
    "    'analysis': {\n",
    "        'image': 'python:3.9-slim',\n",
    "        'mounts': ['/data', '/results'],\n",
    "        'env': {'PYTHONUNBUFFERED': '1'},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nAvailable container templates:\")\n",
    "for name, config in container_configs.items():\n",
    "    print(f\"  {name}: {config['image']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c98948",
   "metadata": {},
   "source": [
    "## Section 11: System Integration and Resource Control\n",
    "\n",
    "### cgroups (Control Groups)\n",
    "- **CPU Limiting**: Restrict CPU usage to specific percentage\n",
    "- **Memory Limits**: Enforce hard memory ceiling\n",
    "- **I/O Limits**: Control disk bandwidth usage\n",
    "- **Process Limits**: Restrict number of processes/threads\n",
    "\n",
    "### NUMA (Non-Uniform Memory Architecture)\n",
    "- **Topology Awareness**: Understand multi-socket systems\n",
    "- **CPU Affinity**: Pin processes to specific cores\n",
    "- **Memory Affinity**: Allocate memory close to CPUs\n",
    "- **Optimal Placement**: Minimize inter-socket traffic\n",
    "\n",
    "### systemd Integration\n",
    "- **Service Management**: Control job lifecycle via systemd\n",
    "- **Resource Units**: Define systemd unit files for jobs\n",
    "- **Dependency Management**: Handle service dependencies\n",
    "- **Auto-restart**: Implement fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9022380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11: System Integration and NUMA Awareness\n",
    "@dataclass\n",
    "class NUMATopology:\n",
    "    \"\"\"NUMA system topology\"\"\"\n",
    "    num_sockets: int\n",
    "    cores_per_socket: int\n",
    "    memory_per_socket_gb: float\n",
    "\n",
    "class NUMAAwareScheduler:\n",
    "    \"\"\"Schedule jobs with NUMA awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, topology: NUMATopology):\n",
    "        self.topology = topology\n",
    "        self.job_placement = {}\n",
    "    \n",
    "    def get_total_cores(self) -> int:\n",
    "        \"\"\"Get total cores in system\"\"\"\n",
    "        return self.topology.num_sockets * self.topology.cores_per_socket\n",
    "    \n",
    "    def get_total_memory_gb(self) -> float:\n",
    "        \"\"\"Get total memory\"\"\"\n",
    "        return self.topology.num_sockets * self.topology.memory_per_socket_gb\n",
    "    \n",
    "    def calculate_affinity(self, job_id: int, cores: int) -> Dict:\n",
    "        \"\"\"Calculate optimal CPU affinity for job\"\"\"\n",
    "        # Try to pack cores within single socket if possible\n",
    "        cores_per_socket = self.topology.cores_per_socket\n",
    "        \n",
    "        if cores <= cores_per_socket:\n",
    "            # Fit in single socket\n",
    "            socket_id = 0\n",
    "            core_list = list(range(cores))\n",
    "        else:\n",
    "            # Span multiple sockets, balance across sockets\n",
    "            socket_id = 0\n",
    "            cores_assigned = []\n",
    "            remaining = cores\n",
    "            \n",
    "            for socket in range(self.topology.num_sockets):\n",
    "                assign = min(remaining, cores_per_socket)\n",
    "                socket_base = socket * cores_per_socket\n",
    "                cores_assigned.extend(range(socket_base, socket_base + assign))\n",
    "                remaining -= assign\n",
    "        \n",
    "        return {\n",
    "            'job_id': job_id,\n",
    "            'cores': cores,\n",
    "            'cpu_affinity': cores_assigned,\n",
    "            'memory_socket': socket_id,\n",
    "        }\n",
    "\n",
    "# Example NUMA topology\n",
    "topology = NUMATopology(\n",
    "    num_sockets=2,\n",
    "    cores_per_socket=16,\n",
    "    memory_per_socket_gb=64.0\n",
    ")\n",
    "scheduler = NUMAAwareScheduler(topology)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NUMA TOPOLOGY AND SCHEDULING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nSystem Topology:\")\n",
    "print(f\"  Sockets: {topology.num_sockets}\")\n",
    "print(f\"  Cores per socket: {topology.cores_per_socket}\")\n",
    "print(f\"  Total cores: {scheduler.get_total_cores()}\")\n",
    "print(f\"  Total memory: {scheduler.get_total_memory_gb():.0f} GB\")\n",
    "print(f\"  Memory per socket: {topology.memory_per_socket_gb:.0f} GB\")\n",
    "\n",
    "# Test job placements\n",
    "test_jobs = [\n",
    "    (1, 8),   # Small job\n",
    "    (2, 16),  # Socket-sized job\n",
    "    (3, 32),  # Multi-socket job\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Job':<6} {'Cores':<8} {'Affinity':<35} {'Socket':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for job_id, cores in test_jobs:\n",
    "    affinity = scheduler.calculate_affinity(job_id, cores)\n",
    "    core_str = f\"[{affinity['cpu_affinity'][:3]}...{affinity['cpu_affinity'][-1:]}]\"\n",
    "    print(f\"{job_id:<6} {cores:<8} {str(core_str):<35} {affinity['memory_socket']}\")\n",
    "\n",
    "# systemd integration example\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"systemd Unit File Example (for HPC job)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "unit_template = \"\"\"[Unit]\n",
    "Description=HPC Job %i\n",
    "After=network-online.target\n",
    "Wants=network-online.target\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "User=hpc_user\n",
    "WorkingDirectory=/home/hpc_user/jobs\n",
    "ExecStart=/usr/bin/python3 /home/hpc_user/jobs/job_%i.py\n",
    "Restart=on-failure\n",
    "RestartSec=10\n",
    "CPUQuota=50%\n",
    "MemoryLimit=4G\n",
    "TimeoutStopSec=30s\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "\"\"\"\n",
    "\n",
    "print(unit_template)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae96f97",
   "metadata": {},
   "source": [
    "## Summary: Complete HPC Job Management System\n",
    "\n",
    "### Key Components Implemented\n",
    "1. **Resource Allocation**: Flexible CPU, memory, GPU allocation\n",
    "2. **Scheduling Algorithms**: FIFO, priority queues, fair-share, backfill\n",
    "3. **Job Dependencies**: DAG-based workflow management\n",
    "4. **Performance Monitoring**: Real-time metrics and anomaly detection\n",
    "5. **Benchmarking**: CPU, memory, I/O performance assessment\n",
    "6. **Parallel Execution**: Multi-core job execution with load balancing\n",
    "7. **ML Optimization**: Runtime prediction and resource recommendations\n",
    "8. **Advanced Scheduling**: Gang scheduling, preemption, backfill\n",
    "9. **Workflow Management**: Complex DAG workflows with retry logic\n",
    "10. **Security & Isolation**: User quotas, containers, audit logging\n",
    "11. **System Integration**: NUMA awareness, cgroups, systemd support\n",
    "\n",
    "### Best Practices\n",
    "- Always monitor job performance\n",
    "- Use resource predictions to optimize allocation\n",
    "- Implement checkpointing for long-running jobs\n",
    "- Balance fairness with utilization\n",
    "- Provide clear feedback to users\n",
    "- Maintain comprehensive audit logs\n",
    "- Support multiple job types and patterns\n",
    "\n",
    "### Future Enhancements\n",
    "- Machine learning-based scheduling optimization\n",
    "- Multi-cluster federation\n",
    "- Advanced visualization dashboards\n",
    "- Energy-aware scheduling\n",
    "- Automatic workload characterization\n",
    "- Real-time job migration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
